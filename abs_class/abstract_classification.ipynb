{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Abstract Classification Report"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### About\r\n",
    "\r\n",
    "This naive bayes algorithm predicts the domain from the following: \r\n",
    "Archaea(A), Bacteria(B), Eukaryota(E) or Virus(V) on the abstracts from research papers \r\n",
    "about proteins taken from the MEDLINE database.\r\n",
    "\r\n",
    "\r\n",
    "### Pre-processing\r\n",
    "\r\n",
    "Since I only use NumPy to manipulate the data, I decided to first read the class and abstract rows in the csv into a list each, then used pop() to remove the headers. To remove the stop words in the abstracts I got a list of stop words and assigned the list a variable. I used split() on the abstracts so that I could iterate through all the words in the abstract and remove all stop words. In the end I am passing a NumPy array of a list of sub lists (each sub list is an abstract) of words. Also passing a NumPy array of classes.\r\n",
    "\r\n",
    "\r\n",
    "### Data representation\r\n",
    "\r\n",
    "I decided to represent my abstract data as a dictionary where we have a word frequency for each word within that abstract. I decided to use dictionaries because it lets me access each word’s frequency very easily by simply using a key/ value for loop and if I want to access a specific abstract, I can just use another for loop to do so.\r\n",
    "\r\n",
    "\r\n",
    "### Method extension\r\n",
    "\r\n",
    "For my method extension I tried a few, like n-grams, TF-IDF and complement naïve bayes. Using n-grams, specifically bigrams, I saw a huge decrease in my model’s accuracy all the way down to 50%. Which I justified by the fact that when you join two terms the likelihood of the same joint term occurring in the data becomes quite rare, so it does not really provide any useful training data for my model. Using TF-IDF I saw my model’s performance increase slightly but by a very small amount. The increase in performance, I justified by the fact that TF-IDF tries to make up for the fact that some words like ‘gene’ for example appear a lot in the different abstracts, so that word does not really help in classification, so TF-IDF makes up for this by assigning it a lower weight than words that do not occur in as many abstracts meaning the word ‘gene’ has a smaller effect on our classification.\r\n",
    "\r\n",
    "Complement Naïve Bayes gave me the best increase in performance, which is because, instead of calculating the conditional probability of a word occurring in the class, it calculates the complement probability of that class i.e., the probability of the word occurring in other classes. At the end we take the class with the lowest value instead of the highest value because the class with the highest value means, the likelihood that, an abstract with those words occurring with that class is highly unlikely. This strategy of taking the complement of a class works especially well with datasets that have imbalanced classes, which our dataset happens to have.\r\n",
    "\r\n",
    "So, the extended method I decided to use was Complement Naïve Bayes because it gave me the highest accuracy out of all the ones that I tried.\r\n",
    "\r\n",
    "\r\n",
    "### Implementation\r\n",
    "For my standard naïve bayes function I passed three arguments the training abstracts, training classes and the test abstracts. I stored each abstract in the training set into its respective class and stored them into list variables. Then found the frequency of each word in each class and stored them into dictionaries of word and frequency pairs. \r\n",
    "\r\n",
    "Then I did the calculations of the conditional probability of each word but iterated through the test abstract instead of the training abstract to avoid making any unnecessary calculations because not all words in the training set are in the test set. I also applied smoothing variables to the equation in case of a word not appearing within the class. I did this calculation for each class and stored the words in a dictionary of word and conditional probability for each class. Then I iterated through each abstract in the test set and each word in the abstract and did the final calculation for predicting the class. To avoid underflow due to multiplying very small numbers I got the log of each value in my prediction calculation and then, finally, I chose the class with the highest value as the classification for that test abstract. \r\n",
    "\r\n",
    "For my complement naïve bayes I created a similar function passing the exact same three arguments. But instead of calculating conditional probabilities I calculated the complement of that class. I did this by getting the total frequency of the word in all other classes’ training set and adding them together along with a smoothing variable and dividing that by the number of words within all other classes’ training set along with a smoothing variable I did this for all other classes. The prediction calculation was the exact same as the standard but instead of getting the max value I got the min because like I mentioned above, in the complement NB the higher-class values mean higher likely hood of those words not appearing within an abstract of that classification.\r\n",
    "\r\n",
    "\r\n",
    "### Performance\r\n",
    "standard NB score: 0.932 / complement NB score: 0.947\r\n",
    " \r\n",
    "standard NB score: 0.929 / complement NB score: 0.947\r\n",
    " \r\n",
    "standard NB score: 0.931 / complement NB score: 0.946\r\n",
    " \r\n",
    "standard NB score: 0.930 / complement NB score: 0.946\r\n",
    " \r\n",
    "standard NB score: 0.929 / complement NB score: 0.946\r\n",
    " \r\n",
    "standard NB mean accuracy: 0.930 std: 0.001\r\n",
    "\r\n",
    "complement NB mean accuracy: 0.946 std: 0.000\r\n",
    "\r\n",
    "After running 10-fold cross validation five times we got a higher score for complement naïve bayes every time compared to the standard one. Overall, the complement naïve bayes gave us 1.6% increase in accuracy.\r\n",
    "\r\n",
    "When running the model on the tst.csv dataset and then uploading it on Kaggle, the standard model had a 92% accuracy score while the complement naïve bayes had a score of 93.6% so again, a 1.6% increase in performance. Therefore, I can say that the extended model performs better than the standard model.\r\n",
    "\r\n",
    "\r\n",
    "### Model Evaluation/Validation\r\n",
    "\r\n",
    "To evaluate my model, I decided to use k-fold cross validation since it splits the data into k-folds so that each fold gets to be in the train and test set. This gives a good estimation of the performance of our model on the entire dataset. I decided to go for 10 k-fold because it splits my data set evenly and is universally regarded as a good number for k-fold due to its not very high bias/ variance. To calculate the accuracy mean I took the mean of the 10 results which is given by a score function which just compares the models’ class predictions to the test classes.\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Abstract Classification Code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import csv\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import math"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing trg.csv file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "classes = []\r\n",
    "abstracts = []\r\n",
    "w_sep = []\r\n",
    "new_words = []\r\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\r\n",
    "with open(\"trg.csv\") as csv_file:\r\n",
    "    read_file = csv.reader(csv_file, delimiter = ',', quotechar='\"')\r\n",
    "    for row in read_file:\r\n",
    "        classes.append(row[1])\r\n",
    "        abstracts.append(row[2])\r\n",
    "    classes.pop(0)\r\n",
    "    abstracts.pop(0)\r\n",
    "    for i in abstracts:\r\n",
    "        spl = i.split()\r\n",
    "        w_sep.append(spl)\r\n",
    "    for i in w_sep:\r\n",
    "        clean = [word for word in i if not word in stopwords]\r\n",
    "        new_words.append(clean)\r\n",
    "np_classes = np.array(classes)\r\n",
    "np_abstracts = np.array(new_words, dtype=object)\r\n",
    "#print(type(np_abstracts[:10]))\r\n",
    "#print(np_classes[:100])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross-validation function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def cross_validation(dataset, k):\r\n",
    "    data = []\r\n",
    "    index = list(range(0, len(dataset)))\r\n",
    "    rand_index = random.sample(index, len(index))\r\n",
    "    np_index = np.array(rand_index)\r\n",
    "    for split in np.split(np_index, k):\r\n",
    "        sp = split.tolist()\r\n",
    "        data.append(sp)\r\n",
    "    for i in range(k):\r\n",
    "        test_index = data[i]\r\n",
    "        train = data.copy()\r\n",
    "        train.pop(i)\r\n",
    "        train_index = [item for sublist in train for item in sublist]\r\n",
    "        yield train_index, test_index"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Turns list of words into a dictionary of frequencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def wordfreq(data):\r\n",
    "    unique, counts = np.unique(data, return_counts=True)\r\n",
    "    freq = dict(zip(unique, counts))\r\n",
    "    return freq"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Standard Naive Bayes function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def naivebayes(X, y, X_test):\r\n",
    "    classes_count = wordfreq(y)\r\n",
    "    values = classes_count.values()\r\n",
    "    total_classes = sum(values)\r\n",
    "    words_A = []\r\n",
    "    words_B = []\r\n",
    "    words_E = []\r\n",
    "    words_V = []\r\n",
    "    all_words = []\r\n",
    "    all_test_words = []\r\n",
    "    cond_prob_A = {}\r\n",
    "    cond_prob_B = {}\r\n",
    "    cond_prob_E = {}\r\n",
    "    cond_prob_V = {}\r\n",
    "    predict = {}\r\n",
    "    prior = {}\r\n",
    "    final_prediction = []\r\n",
    "    for key, value in classes_count.items():\r\n",
    "        prior[key] = value / total_classes    \r\n",
    "        \r\n",
    "    for count, i in enumerate(y):\r\n",
    "        if i == 'A':\r\n",
    "            words_A.extend(X[count])\r\n",
    "        elif i == 'B':\r\n",
    "            words_B.extend(X[count])\r\n",
    "        elif i == 'E':\r\n",
    "            words_E.extend(X[count])\r\n",
    "        elif i == 'V':\r\n",
    "            words_V.extend(X[count])\r\n",
    "        all_words.extend(X[count])\r\n",
    "        \r\n",
    "    all_unique_words = wordfreq(all_words)\r\n",
    "    words_A_count = wordfreq(words_A)\r\n",
    "    words_B_count = wordfreq(words_B)\r\n",
    "    words_E_count = wordfreq(words_E)\r\n",
    "    words_V_count = wordfreq(words_V)\r\n",
    "    \r\n",
    "    for i in X_test:\r\n",
    "        all_test_words.extend(i)\r\n",
    "\r\n",
    "    unique_words_test = wordfreq(all_test_words)\r\n",
    "    for test_key in unique_words_test.keys():\r\n",
    "        value_A = words_A_count.get(test_key)\r\n",
    "        if value_A != None:\r\n",
    "            cond_prob_A[test_key] = value_A + 1 / (len(words_A) + len(all_unique_words))\r\n",
    "        else:\r\n",
    "            cond_prob_A[test_key] = 0 + 1 / (len(words_A) + len(all_unique_words))\r\n",
    "                \r\n",
    "        value_B = words_B_count.get(test_key)\r\n",
    "        if value_B != None:\r\n",
    "            cond_prob_B[test_key] = value_B + 1 / (len(words_B) + len(all_unique_words))\r\n",
    "        else:\r\n",
    "            cond_prob_B[test_key] = 0 + 1 / (len(words_B) + len(all_unique_words))\r\n",
    "                \r\n",
    "        value_E = words_E_count.get(test_key)\r\n",
    "        if value_E != None:\r\n",
    "            cond_prob_E[test_key] = value_E + 1 / (len(words_E) + len(all_unique_words))\r\n",
    "        else:\r\n",
    "            cond_prob_E[test_key] = 0 + 1 / (len(words_E) + len(all_unique_words))\r\n",
    "                \r\n",
    "        value_V = words_V_count.get(test_key)   \r\n",
    "        if value_V != None:\r\n",
    "            cond_prob_V[test_key] = value_V + 1 / (len(words_V) + len(all_unique_words))\r\n",
    "        else:\r\n",
    "            cond_prob_V[test_key] = 0 + 1 / (len(words_V) + len(all_unique_words))\r\n",
    "         \r\n",
    "\r\n",
    "    for abstract in X_test:\r\n",
    "        abs_count = wordfreq(abstract)\r\n",
    "        for key, value in abs_count.items():\r\n",
    "            predict['A'] = predict.get('A', 0) + value * math.log(cond_prob_A.get(key))\r\n",
    "            predict['B'] = predict.get('B', 0) + value * math.log(cond_prob_B.get(key))\r\n",
    "            predict['E'] = predict.get('E', 0) + value * math.log(cond_prob_E.get(key))\r\n",
    "            predict['V'] = predict.get('V', 0) + value * math.log(cond_prob_V.get(key))\r\n",
    "\r\n",
    "        final_A =  predict.get('A') + math.log(prior.get('A'))\r\n",
    "        final_B =  predict.get('B') + math.log(prior.get('B'))\r\n",
    "        final_E =  predict.get('E') + math.log(prior.get('E'))\r\n",
    "        final_V =  predict.get('V') + math.log(prior.get('V'))\r\n",
    "        final = {'A': final_A, 'B': final_B, 'E': final_E, 'V': final_V}    \r\n",
    "        letter = max(final, key=final.get)    \r\n",
    "        final_prediction.extend(letter)\r\n",
    "        predict.clear()\r\n",
    "        \r\n",
    "    return final_prediction"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Complement Naive Bayes function "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def naivebayes_complement(X, y, X_test):\r\n",
    "    classes_count = wordfreq(y)\r\n",
    "    values = classes_count.values()\r\n",
    "    total_classes = sum(values)\r\n",
    "    total_docs = len(X)\r\n",
    "    words_A = []\r\n",
    "    words_B = []\r\n",
    "    words_E = []\r\n",
    "    words_V = []\r\n",
    "    all_words = []\r\n",
    "    all_test_words = []\r\n",
    "    comp_prob_A = {}\r\n",
    "    comp_prob_B = {}\r\n",
    "    comp_prob_E = {}\r\n",
    "    comp_prob_V = {}\r\n",
    "    predict = {}\r\n",
    "    prior = {}\r\n",
    "    final_prediction = []\r\n",
    "    for key, value in classes_count.items():\r\n",
    "        prior[key] = value / total_classes    \r\n",
    "        \r\n",
    "    for count, i in enumerate(y):\r\n",
    "        if i == 'A':\r\n",
    "            words_A.extend(X[count])\r\n",
    "        elif i == 'B':\r\n",
    "            words_B.extend(X[count])\r\n",
    "        elif i == 'E':\r\n",
    "            words_E.extend(X[count])\r\n",
    "        elif i == 'V':\r\n",
    "            words_V.extend(X[count])\r\n",
    "        all_words.extend(X[count])\r\n",
    "        \r\n",
    "    all_unique_words = wordfreq(all_words)\r\n",
    "    words_A_count = wordfreq(words_A)\r\n",
    "    words_B_count = wordfreq(words_B)\r\n",
    "    words_E_count = wordfreq(words_E)\r\n",
    "    words_V_count = wordfreq(words_V)\r\n",
    "    \r\n",
    "    for i in X_test:\r\n",
    "        all_test_words.extend(i)\r\n",
    "        \r\n",
    "    unique_words_test = wordfreq(all_test_words)\r\n",
    "    for test_key in unique_words_test.keys():\r\n",
    "        value_A = words_A_count.get(test_key)\r\n",
    "        if value_A != None:\r\n",
    "            comp_prob_A[test_key] = ((words_B_count.get(test_key, 0) + words_E_count.get(test_key, 0) + words_V_count.get(test_key, 0)) + 1) / ((len(words_B) + len(words_E) + len(words_V)) + len(unique_words_test))\r\n",
    "                \r\n",
    "        value_B = words_B_count.get(test_key)\r\n",
    "        if value_B != None:\r\n",
    "            comp_prob_B[test_key] = ((words_A_count.get(test_key, 0) + words_E_count.get(test_key, 0) + words_V_count.get(test_key, 0)) + 1) / ((len(words_A) + len(words_E) + len(words_V)) + len(unique_words_test))\r\n",
    "                \r\n",
    "        value_E = words_E_count.get(test_key)\r\n",
    "        if value_E != None:\r\n",
    "            comp_prob_E[test_key] = ((words_A_count.get(test_key, 0) + words_B_count.get(test_key, 0) + words_V_count.get(test_key, 0)) + 1) / ((len(words_A) + len(words_B) + len(words_V)) + len(unique_words_test))\r\n",
    "                \r\n",
    "        value_V = words_V_count.get(test_key)   \r\n",
    "        if value_V != None:\r\n",
    "            comp_prob_V[test_key] = ((words_A_count.get(test_key, 0) + words_B_count.get(test_key, 0) + words_E_count.get(test_key, 0)) + 1) / ((len(words_A) + len(words_B) + len(words_E)) + len(unique_words_test))\r\n",
    "         \r\n",
    "\r\n",
    "    for abstract in X_test:\r\n",
    "        abs_count = wordfreq(abstract)\r\n",
    "        for key, value in abs_count.items():\r\n",
    "            prob_A = comp_prob_A.get(key)\r\n",
    "            if prob_A != None:\r\n",
    "                predict['A'] = predict.get('A', 0) + value * math.log(comp_prob_A.get(key))\r\n",
    "            else:\r\n",
    "                predict['A'] = predict.get('A', 0) + 0\r\n",
    "                \r\n",
    "            prob_B = comp_prob_B.get(key)\r\n",
    "            if prob_B != None:\r\n",
    "                predict['B'] = predict.get('B', 0) + value * math.log(comp_prob_B.get(key))\r\n",
    "            else:\r\n",
    "                predict['B'] = predict.get('B', 0) + 0\r\n",
    "                \r\n",
    "            prob_E = comp_prob_E.get(key)\r\n",
    "            if prob_E != None:\r\n",
    "                predict['E'] = predict.get('E', 0) + value * math.log(comp_prob_E.get(key))\r\n",
    "            else:\r\n",
    "                predict['E'] = predict.get('E', 0) + 0\r\n",
    "                \r\n",
    "            prob_V = comp_prob_V.get(key)\r\n",
    "            if prob_V != None:\r\n",
    "                predict['V'] = predict.get('V', 0) + value * math.log(comp_prob_V.get(key))\r\n",
    "            else:\r\n",
    "                predict['V'] = predict.get('V', 0) + 0\r\n",
    "\r\n",
    "        final_A =  predict.get('A') + math.log(prior.get('A'))\r\n",
    "        final_B =  predict.get('B') + math.log(prior.get('B'))\r\n",
    "        final_E =  predict.get('E') + math.log(prior.get('E'))\r\n",
    "        final_V =  predict.get('V') + math.log(prior.get('V'))\r\n",
    "        final = {'A': final_A, 'B': final_B, 'E': final_E, 'V': final_V}    \r\n",
    "        letter = min(final, key=final.get)    \r\n",
    "        final_prediction.extend(letter)\r\n",
    "        predict.clear()\r\n",
    "        \r\n",
    "    return final_prediction"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scoring function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def score(nb, y):\r\n",
    "    n = np.array(nb)\r\n",
    "    correct = (n == y)\r\n",
    "    accuracy = correct.sum() / correct.size\r\n",
    "    \r\n",
    "    return accuracy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Runs 10 fold CV on NB functions and gives the mean accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "total_stand = []\r\n",
    "total_comp = []\r\n",
    "cv_stand = []\r\n",
    "cv_comp = []\r\n",
    "for i in range(5):\r\n",
    "    for train_index, test_index in cross_validation(classes, 10):\r\n",
    "        X_train, X_test = np_abstracts[train_index], np_abstracts[test_index]\r\n",
    "        y_train, y_test = np_classes[train_index], np_classes[test_index]\r\n",
    "        nb_stand = naivebayes(X_train, y_train, X_test)\r\n",
    "        nb_comp = naivebayes_complement(X_train, y_train, X_test)\r\n",
    "        score_stand = score(nb_stand, y_test)\r\n",
    "        score_comp = score(nb_comp, y_test)\r\n",
    "        cv_stand.append(score_stand)\r\n",
    "        cv_comp.append(score_comp)\r\n",
    "    cv_stand_avg = (np.mean(np.array(cv_stand)))\r\n",
    "    cv_comp_avg = (np.mean(np.array(cv_comp)))\r\n",
    "    print(f\"standard NB score: {cv_stand_avg:.3f}\")\r\n",
    "    print(f\"complement NB score: {cv_comp_avg:.3f}\")\r\n",
    "    print(\" \")\r\n",
    "    total_stand.append(cv_stand_avg)\r\n",
    "    total_comp.append(cv_comp_avg)\r\n",
    "    cv_stand.clear()\r\n",
    "    cv_comp.clear()\r\n",
    "overall_avg_stand = np.mean(np.array(total_stand))\r\n",
    "overall_avg_comp = np.mean(np.array(total_comp))\r\n",
    "overall_std_stand = np.std(np.array(total_stand))\r\n",
    "overall_std_comp = np.std(np.array(total_comp))\r\n",
    "print(f\"standard NB mean accuracy: {overall_avg_stand:.3f} std: {overall_std_stand:.3f}\")\r\n",
    "print(f\"complement NB mean accuracy: {overall_avg_comp:.3f} std: {overall_std_comp:.3f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "standard NB score: 0.930\n",
      "complement NB score: 0.946\n",
      " \n",
      "standard NB score: 0.929\n",
      "complement NB score: 0.948\n",
      " \n",
      "standard NB score: 0.929\n",
      "complement NB score: 0.946\n",
      " \n",
      "standard NB score: 0.927\n",
      "complement NB score: 0.943\n",
      " \n",
      "standard NB score: 0.929\n",
      "complement NB score: 0.947\n",
      " \n",
      "standard NB mean accuracy: 0.929 std: 0.001\n",
      "complement NB mean accuracy: 0.946 std: 0.002\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "deca9b341bfe840aacf908b00e24804a7f259194fc988101edf12a473132e09e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}